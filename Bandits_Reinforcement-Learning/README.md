# Theory of Multi-armed Bandits and Reinforcement Learning

This is Berkeley course, taught by Jiantao Jiao, available [here](https://people.eecs.berkeley.edu/~jiantao/2902021spring/).

### Description

This course will cover the cutting edge of multi-armed bandits and reinforcement learning theory. We will talk about stochastic and adversarial bandits, the best-of-both-worlds phenomenon, the UCB and EXP3 algorithms and their variations, lower bound techniques, exploration bonus design for contextual bandits and RL, the connections between Thompson sampling and frequentist framework, non-asymptotic analysis of TD learning, policy gradient, and Q learning, fundamental limits of online and offline RL, policy evaluation v.s. optimization, imitation learning and inverse RL, as well as theory building for practical algorithms to bridge the gap between theory and practice.

### Material

The material consists of several lecture notes.

- [X] Lecture 1: Introduction to Bandits and Reinforcement Learning
- [X] Lecture 2: Analysis of finite-arm i.i.d.-reward bandit
	- Explore then commit algorithm
- [X] Lecture 3: Explore then commit and successive elimination
	- Succesive elimination algorithm
- [X] Lecture 4: Analysis of Successive Elimination and UCB Algorithm
	- UCB1 algorithm
	- Phased successive elimination algorithm
- [ ] Lecture 5: Minimax Lower Bound for Finite-Arm Bandit Algorithms
- [ ] Lecture 6: Minimax Lower Bound and Thompson Sampling
